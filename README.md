# ee5904-homework1-signal-flow-graph-of-the-perceptron-solved
**TO GET THIS SOLUTION VISIT:** [EE5904 Homework1-signal-flow graph of the perceptron Solved](https://www.ankitcodinghub.com/product/ee5904-homework1-signal-flow-graph-of-the-perceptron-solved/)


---

📩 **If you need this solution or have special requests:** **Email:** ankitcoding@gmail.com  
📱 **WhatsApp:** +1 419 877 7882  
📄 **Get a quote instantly using this form:** [Ask Homework Questions](https://www.ankitcodinghub.com/services/ask-homework-questions/)

*We deliver fast, professional, and affordable academic help.*

---

<h2>Description</h2>



<div class="kk-star-ratings kksr-auto kksr-align-center kksr-valign-top" data-payload="{&quot;align&quot;:&quot;center&quot;,&quot;id&quot;:&quot;60465&quot;,&quot;slug&quot;:&quot;default&quot;,&quot;valign&quot;:&quot;top&quot;,&quot;ignore&quot;:&quot;&quot;,&quot;reference&quot;:&quot;auto&quot;,&quot;class&quot;:&quot;&quot;,&quot;count&quot;:&quot;2&quot;,&quot;legendonly&quot;:&quot;&quot;,&quot;readonly&quot;:&quot;&quot;,&quot;score&quot;:&quot;5&quot;,&quot;starsonly&quot;:&quot;&quot;,&quot;best&quot;:&quot;5&quot;,&quot;gap&quot;:&quot;4&quot;,&quot;greet&quot;:&quot;Rate this product&quot;,&quot;legend&quot;:&quot;5\/5 - (2 votes)&quot;,&quot;size&quot;:&quot;24&quot;,&quot;title&quot;:&quot;EE5904 Homework1-signal-flow graph of the perceptron Solved&quot;,&quot;width&quot;:&quot;138&quot;,&quot;_legend&quot;:&quot;{score}\/{best} - ({count} {votes})&quot;,&quot;font_factor&quot;:&quot;1.25&quot;}">

<div class="kksr-stars">

<div class="kksr-stars-inactive">
            <div class="kksr-star" data-star="1" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" data-star="2" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" data-star="3" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" data-star="4" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" data-star="5" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
    </div>

<div class="kksr-stars-active" style="width: 138px;">
            <div class="kksr-star" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
    </div>
</div>


<div class="kksr-legend" style="font-size: 19.2px;">
            5/5 - (2 votes)    </div>
    </div>
<strong>Q1</strong>.

Consider the signal-flow graph of the perceptron shown in the above figure. The activation function, ϕ( )<em>v </em>, where v is the induced local field, can be designed by the user. If the activation function is chosen as hard limiter (i.e. step function), then it becomes the classical perceptron, and the decision boundary is shown to be a hyperplane.&nbsp; In this problem, let’s explore other choices of the activation function, and its effect on the decision boundary.&nbsp; Let’s assume that the classification decision made by the perceptron is simply a threshold rule defined as follows:

<em>Observation vector belongs to class C<sub>1</sub> if the output y &gt;</em>ξ<em>, where</em>ξ<em>is a userdefined threshold; otherwise, x belongs to class C<sub>2</sub>. </em>

Consider the following three choices of activation function:

<ul>
<li>The activation function is a linear function: ϕ( )<em>v </em>= +<em>av b</em>;</li>
<li>The activation function is the logistic function: ϕ( )<em>v </em>= <sub>−</sub><sub>2<em>v </em></sub>;</li>
</ul>
1+<em>e</em>

<em>v</em>2

−

<ul>
<li>The activation function is the Bell-shaped Gaussian function: ϕ( )<em>v </em>=<em>e </em><sup>2 </sup>.</li>
</ul>
For each case, investigate whether the resulting decision boundary is a hyper-plane or not.

&nbsp;

<strong>Q2</strong>.

Consider the logic function, EXCLUSIVE OR (XOR).&nbsp; Truth Table of XOR

<table width="139">
<tbody>
<tr>
<td width="35">x<sub>1</sub></td>
<td width="26">0</td>
<td width="26">1</td>
<td width="26">0</td>
<td width="26">1</td>
</tr>
<tr>
<td width="35">x<sub>2</sub></td>
<td width="26">0</td>
<td width="26">0</td>
<td width="26">1</td>
<td width="26">1</td>
</tr>
<tr>
<td width="35"><em>y </em></td>
<td width="26">0</td>
<td width="26">1</td>
<td width="26">1</td>
<td width="26">0</td>
</tr>
</tbody>
</table>
&nbsp;

It is well known that the XOR problem is not linearly separable. It seems obvious by visually checking, which however cannot be accepted as mathematical proof. Therefore, please supply a rigorous mathematical proof for this statement.

<strong>Q3</strong>.

The perceptron could be used to perform numerous logic functions, such as AND, OR, COMPLEMENT and NAND function, whose truth tables are tabulated as follows respectively.

&nbsp;

<table width="428">
<tbody>
<tr>
<td width="35">x1</td>
<td width="35">0</td>
<td width="35">0</td>
<td width="35">1</td>
<td width="35">1</td>
<td rowspan="3" width="81"></td>
<td width="36">x1</td>
<td width="34">0</td>
<td width="35">0</td>
<td width="34">1</td>
<td width="35">1</td>
</tr>
<tr>
<td width="35">x2</td>
<td width="35">0</td>
<td width="35">1</td>
<td width="35">0</td>
<td width="35">1</td>
<td width="36">x2</td>
<td width="34">0</td>
<td width="35">1</td>
<td width="34">0</td>
<td width="35">1</td>
</tr>
<tr>
<td width="35">y</td>
<td width="35">0</td>
<td width="35">0</td>
<td width="35">0</td>
<td width="35">1</td>
<td width="36">y</td>
<td width="34">0</td>
<td width="35">1</td>
<td width="34">1</td>
<td width="35">1</td>
</tr>
</tbody>
</table>
&nbsp;

<h1><sub>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </sub>AND &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; OR</h1>
&nbsp;

<table width="395">
<tbody>
<tr>
<td width="163">&nbsp;

<table width="104">
<tbody>
<tr>
<td width="35">x</td>
<td width="35">0</td>
<td width="35">1</td>
</tr>
<tr>
<td width="35">y</td>
<td width="35">1</td>
<td width="35">0</td>
</tr>
</tbody>
</table>
</td>
<td width="232">&nbsp;

<table width="174">
<tbody>
<tr>
<td width="35">x1</td>
<td width="35">0</td>
<td width="35">0</td>
<td width="35">1</td>
<td width="35">1</td>
</tr>
<tr>
<td width="35">x2</td>
<td width="35">0</td>
<td width="35">1</td>
<td width="35">0</td>
<td width="35">1</td>
</tr>
<tr>
<td width="35">y</td>
<td width="35">1</td>
<td width="35">1</td>
<td width="35">1</td>
<td width="35">0</td>
</tr>
</tbody>
</table>
</td>
</tr>
</tbody>
</table>
<h1>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; COMPLEMENT &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; NAND</h1>
&nbsp;

a). Demonstrate the implementation of the logic functions AND, OR, COMPLEMENT and NAND with selection of weights by off-line calculations.

<ul>
<li>Marks)</li>
</ul>
b). Demonstrate the implementation of the logic functions AND, OR, COMPLEMENT and NAND with selection of weights by learning procedure. Suppose initial weights are chosen randomly and learning rate &nbsp;is 1.0. Plot out the trajectories of the weights for each case. Compare the results with those obtained in (a). Try other learning rates, and report your observations with different learning rates.

<ul>
<li>Marks)</li>
</ul>
c). What would happen if the perceptron is applied to implement the EXCLUSIVE OR function with selection of weights by learning procedure? Suppose initial weight is chosen randomly and learning rate &nbsp;is 1.0. Do the computer experiment and explain your finding.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (3 Marks) <strong>Q4.&nbsp;</strong>

Single layer perceptron with pure linear activation function can be used to fit a linear model to a set of input-output pairs. Suppose that we are given the following pairs:

{(0,0.5), (0.8, 1), (1.6, 4), (3, 5), (4.0, 6), (5.0, 9)} and a single linear neuron as shown in the following figure.

&nbsp;

&nbsp;

a). Find the solution of w and b using the standard linear least-squares (LLS) method. Plot out the fitting result.

&nbsp;

b). Suppose that initial weight is chosen randomly and learning rate &nbsp;is 0.01. Find the solution of w and b using the least-mean-square (LMS) algorithm for 100 epochs. Plot out the fitting result and the trajectories of the weights versus learning steps. Will the weights converge?

&nbsp;

c). Compare the results obtained by LLS and the LMS methods.

&nbsp;

<ol>
<li>d) Repeat the simulation study in b) with different learning rates , and explain your findings.</li>
</ol>
&nbsp;

<strong>&nbsp;</strong>

<strong>Q5.&nbsp;</strong>

&nbsp;

Consider that we are trying to fit a linear model to a set of input-output pairs (x(1), d(1)), (x(2), d(2)) …, (x(n), d(n)) observed in an interval of duration n, where input x is mdimensional vector, 𝑥𝑥 = [𝑥𝑥<sub>1</sub>, 𝑥𝑥<sub>2</sub>, ⋯ , 𝑥𝑥<sub>𝑚𝑚</sub>]<sup>𝑇𝑇</sup> . The linear model takes the following form:

&nbsp;

𝑦𝑦(𝑥𝑥) = 𝑤𝑤<sub>1</sub>𝑥𝑥<sub>1 </sub>+ 𝑤𝑤<sub>2</sub>𝑥𝑥<sub>2 </sub>+ ⋯ + 𝑤𝑤<sub>𝑚𝑚</sub>𝑥𝑥<sub>𝑚𝑚 </sub>= 𝑤𝑤<sup>𝑇𝑇</sup>𝑥𝑥

&nbsp;

Derive the formula to calculate the optimal parameter w* such that the following cost&nbsp; function J(w) is minimized.

<em>n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </em><sub>2&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </sub><em>n</em>

<em>J w</em>( ) =∑<em>i</em>=1 <em>r i e i</em>( ) ( ) =∑<em>i</em>=1 <em>r i</em>( )( ( )<em>d i </em>− <em>y</em>(x( )))<em>i&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </em><sup>2</sup>

where r(i)&gt;0 are the weighting factors for each output error e(i).

&nbsp;
